{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoCH3rZF+postP75aoQe7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MocT117/Another-one-/blob/master/Untitled44.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjPi_DS4WaRV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Pronóstico semanal por material usando el plan del diagnóstico (barra de progreso)\n",
        "con TODOS los parches:\n",
        "- Ventana de entrenamiento reciente\n",
        "- TSB con parámetros adaptativos por recencia\n",
        "- ADIDA con k sesgado por recencia (y desagregación correcta)\n",
        "- Penalización ADIDA para MTO\n",
        "- Ajuste universal por recencia (down-weight y obsolescencia)\n",
        "\n",
        "Salidas:\n",
        "  outputs/forecast_weekly.csv\n",
        "  outputs/forecast_summary.csv\n",
        "  outputs/forecast_summary.xlsx (se intenta abrir)\n",
        "\"\"\"\n",
        "\n",
        "import os, sys, warnings, math, subprocess\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "\n",
        "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
        "warnings.simplefilter(\"ignore\", FutureWarning)\n",
        "\n",
        "# ===================== PARÁMETROS =====================\n",
        "OUTDIR = \"outputs\"\n",
        "\n",
        "# Plan (diagnóstico)\n",
        "PLAN_XLSX         = r\"C:\\Users\\k1021104\\outputs\\diagnostico_demanda_final_completo.xlsx\"\n",
        "PLAN_CSV_FALLBACK = r\"C:\\Users\\k1021104\\outputs\\diagnostico_demanda_final_completo.csv\"\n",
        "PLAN_SHEET        = 0\n",
        "\n",
        "# Series semanales\n",
        "USE_WEEKLY_PARQUET = True\n",
        "WEEKLY_PARQUET     = os.path.join(OUTDIR, \"weekly_demand.parquet\")\n",
        "\n",
        "# Excel crudo (fallback)\n",
        "INPUT_XLSX = r\"C:\\Users\\k1021104\\OneDrive - Krones AG\\Desktop\\FollowupLayouts\\MaterialesULT.XLSX\"\n",
        "SHEETS     = None\n",
        "\n",
        "# Frecuencia y horizonte\n",
        "FREQ        = \"W-SUN\"\n",
        "DEFAULT_M   = 52\n",
        "H           = 8\n",
        "\n",
        "# SARIMA grid compacto\n",
        "ORDERS      = [(0,1,1), (1,1,0), (1,1,1)]\n",
        "SEAS_ORDERS = [(0,1,1), (1,1,0)]\n",
        "\n",
        "# Recencia (universal)\n",
        "RECENCY_HALFLIFE_ADIs = 1.0     # cada +1×ADI extra reduce 50%\n",
        "OBSOLETE_RATIO        = 3.0     # ratio >= 3 => 0\n",
        "TAG_RECENCY_SUFFIX    = \"+REC\"  # marca en model_used cuando se aplica\n",
        "\n",
        "# Ventana de entrenamiento (reciente)\n",
        "TRAIN_WINDOW = 104  # semanas (ajusta a 78/156 si deseas)\n",
        "\n",
        "# Penalización ADIDA para MTO\n",
        "MTO_MAX_ORDERS     = 9        # <=9 órdenes históricas => MTO\n",
        "MTO_ADIDA_GAMMA    = 0.80     # factor multiplicativo (20% reducción)\n",
        "MTO_COL_NAME       = \"MTS/MTO\"  # si existe en plan\n",
        "\n",
        "# ===================== UTILIDADES BÁSICAS =====================\n",
        "def open_in_excel(path: Path):\n",
        "    try:\n",
        "        if sys.platform.startswith(\"win\"):\n",
        "            os.startfile(str(path))  # type: ignore[attr-defined]\n",
        "        elif sys.platform == \"darwin\":\n",
        "            subprocess.Popen([\"open\", str(path)])\n",
        "        else:\n",
        "            subprocess.Popen([\"xdg-open\", str(path)])\n",
        "        print(f\"[OK] Abierto en Excel: {path.resolve()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[AVISO] No se pudo abrir automáticamente: {e}\")\n",
        "\n",
        "def safe_to_csv(df: pd.DataFrame, path: Path) -> Path:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"[OK] CSV guardado en: {path.resolve()}\")\n",
        "        return path\n",
        "    except PermissionError:\n",
        "        alt = path.with_stem(path.stem + \"_ALT\")\n",
        "        df.to_csv(alt, index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"[AVISO] Archivo abierto. Guardado como: {alt.resolve()}\")\n",
        "        return alt\n",
        "\n",
        "def norm_id(series, width):\n",
        "    s = series.astype(str).str.strip()\n",
        "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "    if width is not None:\n",
        "        mask_num = s.str.fullmatch(r\"\\d+\")\n",
        "        s.loc[mask_num] = s.loc[mask_num].str.zfill(int(width))\n",
        "    return s\n",
        "\n",
        "def infer_width_from_plan(plan: pd.DataFrame):\n",
        "    m = plan[\"Material\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "    m_num = m[m.str.fullmatch(r\"\\d+\")]\n",
        "    return int(m_num.str.len().max()) if not m_num.empty else None\n",
        "\n",
        "def _trim(y: pd.Series, w=TRAIN_WINDOW) -> pd.Series:\n",
        "    return y.iloc[-w:] if (w and len(y) > w) else y\n",
        "\n",
        "# ===================== CARGA DE SERIES =====================\n",
        "def build_weekly_from_excel(df, date_col, qty_col, material_col, freq=FREQ):\n",
        "    df = df[[date_col, qty_col, material_col]].copy()\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
        "    df[qty_col]  = pd.to_numeric(df[qty_col], errors=\"coerce\").fillna(0)\n",
        "    df = df.dropna(subset=[date_col, material_col])\n",
        "\n",
        "    out = {}\n",
        "    n_mats = df[material_col].nunique()\n",
        "    for mat, g in tqdm(df.groupby(material_col), total=n_mats, desc=\"Construyendo series (Excel)\", unit=\"SKU\"):\n",
        "        s = g.set_index(date_col)[qty_col].resample(freq).sum()\n",
        "        if s.empty:\n",
        "            continue\n",
        "        s = s.asfreq(freq, fill_value=0).astype(float)\n",
        "        out[str(mat)] = s\n",
        "    return out\n",
        "\n",
        "def load_weekly_series():\n",
        "    weekly = {}\n",
        "    if USE_WEEKLY_PARQUET and os.path.exists(WEEKLY_PARQUET):\n",
        "        dfw = pd.read_parquet(WEEKLY_PARQUET)\n",
        "        dfw[\"material\"] = dfw[\"material\"].astype(str)\n",
        "        n_mats = dfw[\"material\"].nunique()\n",
        "        for mat, g in tqdm(dfw.groupby(\"material\"), total=n_mats, desc=\"Construyendo series (parquet)\", unit=\"SKU\"):\n",
        "            s = pd.Series(g[\"qty\"].values, index=pd.DatetimeIndex(g[\"week\"])).asfreq(FREQ, fill_value=0).astype(float)\n",
        "            weekly[str(mat)] = s\n",
        "        print(f\"[INFO] Series desde parquet: {len(weekly):,} materiales.\")\n",
        "        return weekly\n",
        "\n",
        "    print(\"[INFO] No hay weekly_demand.parquet; leyendo Excel crudo…\")\n",
        "    xl = pd.read_excel(INPUT_XLSX, sheet_name=SHEETS, dtype=str)\n",
        "    df = pd.concat(xl.values(), ignore_index=True) if isinstance(xl, dict) else xl\n",
        "    if 'Material' not in df.columns:\n",
        "        for alt in ['Materiales','MATNR','matnr','material']:\n",
        "            if alt in df.columns:\n",
        "                df = df.rename(columns={alt:'Material'})\n",
        "                break\n",
        "    weekly = build_weekly_from_excel(df, \"Fecha del documento\", \"Cantidad de pedido\", \"Material\", FREQ)\n",
        "    print(f\"[INFO] Series construidas desde Excel: {len(weekly):,} materiales.\")\n",
        "    return weekly\n",
        "\n",
        "# ===================== MODELOS =====================\n",
        "def fc_naive_seasonal(y_train, h, m=DEFAULT_M):\n",
        "    if len(y_train) < m:\n",
        "        val = float(y_train.iloc[-1]) if len(y_train) else 0.0\n",
        "        return np.repeat(val, h), {\"model\":\"Naive\",\"m\":m}\n",
        "    last_cycle = y_train.iloc[-m:]\n",
        "    reps = int(np.ceil(h / m))\n",
        "    yhat = np.tile(last_cycle.values, reps)[:h]\n",
        "    return yhat, {\"model\":\"Naive\",\"m\":m}\n",
        "\n",
        "def fc_ets(y_train, h, m=DEFAULT_M):\n",
        "    y = y_train.values\n",
        "    best_fit, best_aic, best_spec = None, None, None\n",
        "    use_seasonal = (len(y_train) >= 2*m) and (np.any(y > 0))\n",
        "    specs = []\n",
        "    if use_seasonal:\n",
        "        specs += [\n",
        "            dict(trend=\"add\", damped_trend=True,  seasonal=\"add\", seasonal_periods=m),\n",
        "            dict(trend=\"add\", damped_trend=False, seasonal=\"add\", seasonal_periods=m),\n",
        "        ]\n",
        "    specs += [\n",
        "        dict(trend=\"add\", damped_trend=True,  seasonal=None),\n",
        "        dict(trend=\"add\", damped_trend=False, seasonal=None),\n",
        "        dict(trend=None,  damped_trend=False, seasonal=None),\n",
        "    ]\n",
        "    for sp in specs:\n",
        "        try:\n",
        "            fit = ExponentialSmoothing(y_train, **sp).fit(optimized=True, use_brute=True)\n",
        "            aic = fit.aic\n",
        "            if (best_aic is None) or (aic < best_aic):\n",
        "                best_aic, best_fit, best_spec = aic, fit, sp\n",
        "        except Exception:\n",
        "            continue\n",
        "    if best_fit is None:\n",
        "        yhat, meta = fc_naive_seasonal(y_train, h, m)\n",
        "        meta[\"fallback\"] = \"ETS->Naive\"\n",
        "        return yhat, meta\n",
        "    return best_fit.forecast(h), {\"model\":\"ETS\",\"spec\":best_spec,\"m\":m}\n",
        "\n",
        "def _croston_update(y, alpha=0.1):\n",
        "    Q, Z, pZ = 0.0, 0.0, 1\n",
        "    first = True\n",
        "    for x in y:\n",
        "        if x > 0:\n",
        "            if first:\n",
        "                Q, Z = x, pZ; first = False\n",
        "            else:\n",
        "                Q = Q + alpha*(x - Q)\n",
        "                Z = Z + alpha*(pZ - Z)\n",
        "            pZ = 1\n",
        "        else:\n",
        "            pZ += 1\n",
        "    return Q, Z\n",
        "\n",
        "def fc_croston_sba(y_train, h, alpha=0.1):\n",
        "    Q, Z = _croston_update(y_train.values, alpha=alpha)\n",
        "    if Z == 0:\n",
        "        return np.zeros(h), {\"model\":\"CrostonSBA\",\"alpha\":alpha,\"Q\":Q,\"Z\":Z}\n",
        "    yhat = (1 - alpha/2.0) * (Q / Z)\n",
        "    return np.repeat(yhat, h), {\"model\":\"CrostonSBA\",\"alpha\":alpha,\"Q\":Q,\"Z\":Z}\n",
        "\n",
        "def fc_tsb(y_train, h, alpha=0.1, beta=0.1):\n",
        "    y = y_train.values\n",
        "    p, q, seen = 0.0, 0.0, False\n",
        "    for x in y:\n",
        "        I = 1.0 if x > 0 else 0.0\n",
        "        p = p + alpha*(I - p)\n",
        "        if I == 1:\n",
        "            if not seen: q = x; seen = True\n",
        "            else:       q = q + beta*(x - q)\n",
        "    return np.repeat(p*q, h), {\"model\":\"TSB\",\"alpha\":alpha,\"beta\":beta,\"p_hat\":p,\"q_hat\":q}\n",
        "\n",
        "def fc_sarima(y_train, h, m=DEFAULT_M):\n",
        "    if len(y_train) < (m + 24):\n",
        "        yhat, meta = fc_naive_seasonal(y_train, h, m)\n",
        "        meta[\"fallback\"] = \"SARIMA->Naive\"\n",
        "        return yhat, meta\n",
        "    best_fit, best_aic, best_cfg = None, None, None\n",
        "    for (p,d,q) in ORDERS:\n",
        "        for (P,D,Q) in SEAS_ORDERS:\n",
        "            try:\n",
        "                fit = SARIMAX(y_train, order=(p,d,q),\n",
        "                              seasonal_order=(P,D,Q,m),\n",
        "                              enforce_stationarity=False,\n",
        "                              enforce_invertibility=False).fit(disp=False)\n",
        "                aic = fit.aic\n",
        "                if (best_aic is None) or (aic < best_aic):\n",
        "                    best_aic, best_fit, best_cfg = aic, fit, {\"order\":(p,d,q), \"seasonal_order\":(P,D,Q,m)}\n",
        "            except Exception:\n",
        "                continue\n",
        "    if best_fit is None:\n",
        "        yhat, meta = fc_naive_seasonal(y_train, h, m)\n",
        "        meta[\"fallback\"] = \"SARIMA->Naive\"\n",
        "        return yhat, meta\n",
        "    return best_fit.forecast(h), {\"model\":\"SARIMA\",\"cfg\":best_cfg}\n",
        "\n",
        "# ====== ADIDA (con desagregación correcta) ======\n",
        "def fc_adida(y_weekly, h, base_model=\"ETS\", m=DEFAULT_M, ks=(4,8,13), zero_ratio_target=0.30):\n",
        "    y = y_weekly.astype(float).copy()\n",
        "\n",
        "    def aggregate_by_k(series, k):\n",
        "        n = len(series)\n",
        "        nb = int(math.ceil(n / k))\n",
        "        vals = []\n",
        "        for b in range(nb):\n",
        "            start = b*k\n",
        "            end   = min((b+1)*k, n)\n",
        "            vals.append(series.iloc[start:end].sum())\n",
        "        idx = pd.date_range(series.index[0], periods=nb, freq=f\"{k}W-SUN\")\n",
        "        return pd.Series(vals, index=idx)\n",
        "\n",
        "    # elegir mejor k (simple) en el orden recibido\n",
        "    best_k, best_sr, best_agg = None, None, None\n",
        "    for k in ks:\n",
        "        agg = aggregate_by_k(y, k)\n",
        "        zero_ratio = float((agg <= 0).sum() / len(agg)) if len(agg) > 0 else 1.0\n",
        "        pos_count  = int((agg > 0).sum())\n",
        "        if (best_k is None) and (zero_ratio <= zero_ratio_target or pos_count >= 6):\n",
        "            best_k, best_sr, best_agg = k, agg, (zero_ratio, pos_count)\n",
        "    if best_k is None:\n",
        "        best_k  = ks[-1]\n",
        "        best_sr = aggregate_by_k(y, best_k)\n",
        "        best_agg = ((best_sr <= 0).sum()/len(best_sr), int((best_sr > 0).sum()))\n",
        "\n",
        "    # pronóstico en agregado\n",
        "    if base_model.upper() == \"ETS\":\n",
        "        yhat_agg, meta = fc_ets(best_sr, math.ceil(h / best_k), m=max(1, int(m/best_k)))\n",
        "    elif base_model.upper() == \"SARIMA\":\n",
        "        yhat_agg, meta = fc_sarima(best_sr, math.ceil(h / best_k), m=max(1, int(m/best_k)))\n",
        "    else:\n",
        "        yhat_agg, meta = fc_naive_seasonal(best_sr, math.ceil(h / best_k), m=max(1, int(m/best_k)))\n",
        "\n",
        "    # desagregar (repartir el bloque entre k semanas)\n",
        "    yhat_agg = np.asarray(yhat_agg, dtype=float)\n",
        "    yhat_w   = np.repeat(yhat_agg / best_k, best_k)[:h]\n",
        "    yhat_w   = np.clip(yhat_w, 0.0, None)\n",
        "\n",
        "    meta.update({\"model\": f\"ADIDA({base_model})\", \"k\": best_k,\n",
        "                 \"zero_ratio_agg\": best_agg[0], \"pos_agg\": best_agg[1]})\n",
        "    return yhat_w, meta\n",
        "\n",
        "# ===================== RECENCIA & ADAPTATIVOS =====================\n",
        "def compute_recency_from_series(y: pd.Series, adi_fallback=None):\n",
        "    last_pos_idx = y[y > 0].index.max() if (y > 0).any() else None\n",
        "    last_index   = y.index.max()\n",
        "    weeks_since_last = float(((last_index - last_pos_idx).days / 7)) if last_pos_idx is not None else float('inf')\n",
        "    total_weeks = max(1, len(y))\n",
        "    pos_weeks   = int((y > 0).sum())\n",
        "    adi = (total_weeks / pos_weeks) if pos_weeks > 0 else (adi_fallback if adi_fallback else float('inf'))\n",
        "    ratio = weeks_since_last / adi if (adi and adi > 0 and np.isfinite(adi)) else float('inf')\n",
        "\n",
        "    if ratio <= 1.0:\n",
        "        weight = 1.0\n",
        "    else:\n",
        "        decay_power = (ratio - 1.0) / max(RECENCY_HALFLIFE_ADIs, 1e-9)\n",
        "        weight = 0.5 ** decay_power\n",
        "    if ratio >= OBSOLETE_RATIO:\n",
        "        weight = 0.0\n",
        "    return ratio, weight\n",
        "\n",
        "def adaptive_tsb_params(recency_ratio: float):\n",
        "    if not np.isfinite(recency_ratio):\n",
        "        return 0.05, 0.05\n",
        "    if recency_ratio <= 0.5:\n",
        "        return 0.30, 0.30\n",
        "    elif recency_ratio <= 1.0:\n",
        "        return 0.20, 0.20\n",
        "    elif recency_ratio <= 1.5:\n",
        "        return 0.15, 0.15\n",
        "    else:\n",
        "        return 0.08, 0.08\n",
        "\n",
        "def choose_adida_ks(recency_ratio: float):\n",
        "    if not np.isfinite(recency_ratio):\n",
        "        return (13, 8, 4)\n",
        "    if recency_ratio <= 0.75:\n",
        "        return (4, 8, 13)\n",
        "    elif recency_ratio <= 1.5:\n",
        "        return (8, 4, 13)\n",
        "    else:\n",
        "        return (13, 8, 4)\n",
        "\n",
        "def is_mto(plan_row: pd.Series) -> bool:\n",
        "    # Preferimos flag explícito si viene\n",
        "    if MTO_COL_NAME in plan_row.index:\n",
        "        if str(plan_row[MTO_COL_NAME]).upper() == \"MTO\":\n",
        "            return True\n",
        "    # o por conteo de órdenes si existe\n",
        "    for col in [\"n_orders_orig\", \"n_pedidos\", \"orders_count\"]:\n",
        "        if col in plan_row.index:\n",
        "            try:\n",
        "                return float(plan_row[col]) <= MTO_MAX_ORDERS\n",
        "            except Exception:\n",
        "                pass\n",
        "    return False\n",
        "\n",
        "# ===================== PLAN I/O =====================\n",
        "def load_plan_any(path_xlsx, sheet=0, csv_fallback=None):\n",
        "    if os.path.exists(path_xlsx):\n",
        "        plan = pd.read_excel(path_xlsx, sheet_name=sheet, dtype=str)\n",
        "        src = \"xlsx\"\n",
        "    elif csv_fallback and os.path.exists(csv_fallback):\n",
        "        plan = pd.read_csv(csv_fallback, dtype=str, encoding=\"utf-8-sig\")\n",
        "        src = \"csv\"\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No se encontró el plan en:\\n- {path_xlsx}\\n- {csv_fallback}\")\n",
        "    if \"Material\" not in plan.columns:\n",
        "        raise KeyError(\"El plan no contiene la columna 'Material'.\")\n",
        "    return plan, src\n",
        "\n",
        "def normalize_models_list(primary, candidates):\n",
        "    norm = {\n",
        "        \"ETS\":\"ETS\", \"Holt-Winters\":\"ETS\",\n",
        "        \"SARIMA\":\"SARIMA\",\n",
        "        \"TSB\":\"TSB\",\n",
        "        \"CrostonSBA\":\"CrostonSBA\",\"Croston\":\"CrostonSBA\",\n",
        "        \"NaiveEstacional\":\"Naive\",\"Naive\":\"Naive\",\n",
        "        \"ADIDA\":\"ADIDA\"\n",
        "    }\n",
        "    lst = []\n",
        "    if isinstance(primary, str) and primary.strip():\n",
        "        lst.append(norm.get(primary.strip(), primary.strip()))\n",
        "    if isinstance(candidates, str) and candidates.strip():\n",
        "        for tok in candidates.split(\";\"):\n",
        "            tok = tok.strip()\n",
        "            if tok:\n",
        "                lst.append(norm.get(tok, tok))\n",
        "    seen = set(); out = []\n",
        "    for m in lst:\n",
        "        if m not in seen:\n",
        "            out.append(m); seen.add(m)\n",
        "    if not out:\n",
        "        out = [\"ETS\",\"SARIMA\",\"TSB\",\"CrostonSBA\",\"Naive\"]\n",
        "    return out\n",
        "\n",
        "# ===================== ORQUESTADOR =====================\n",
        "def forecast_one(y: pd.Series, plan_row: pd.Series, models: list, m: int,\n",
        "                 low_data_flag: bool, forced_intermittent: bool,\n",
        "                 apply_uplift: bool, uplift_percent: float):\n",
        "    \"\"\"\n",
        "    Devuelve yhat (np.array) y meta(dict).\n",
        "    Aplica:\n",
        "      - ventana de entrenamiento\n",
        "      - TSB adaptativo\n",
        "      - ADIDA ks por recencia\n",
        "      - penalización ADIDA para MTO\n",
        "    \"\"\"\n",
        "    # recencia (calculada sobre la serie completa)\n",
        "    rec_ratio, rec_weight = compute_recency_from_series(y)\n",
        "\n",
        "    tried = []\n",
        "    models_ord = models[:]\n",
        "    if low_data_flag and \"ADIDA\" not in models_ord:\n",
        "        models_ord = [\"ADIDA\"] + models_ord\n",
        "    if forced_intermittent:\n",
        "        front = [mm for mm in [\"TSB\",\"CrostonSBA\"] if mm in models_ord]\n",
        "        rest  = [mm for mm in models_ord if mm not in front]\n",
        "        models_ord = front + rest\n",
        "\n",
        "    for mod in models_ord:\n",
        "        try:\n",
        "            y_in = _trim(y, TRAIN_WINDOW)\n",
        "\n",
        "            if mod == \"ETS\":\n",
        "                yhat, meta = fc_ets(y_in, H, m)\n",
        "            elif mod == \"SARIMA\":\n",
        "                yhat, meta = fc_sarima(y_in, H, m)\n",
        "            elif mod == \"TSB\":\n",
        "                a, b = adaptive_tsb_params(rec_ratio)\n",
        "                yhat, meta = fc_tsb(y_in, H, alpha=a, beta=b)\n",
        "                meta[\"alpha\"], meta[\"beta\"] = a, b\n",
        "            elif mod == \"CrostonSBA\":\n",
        "                yhat, meta = fc_croston_sba(y_in, H, alpha=0.1)\n",
        "            elif mod == \"Naive\":\n",
        "                yhat, meta = fc_naive_seasonal(y_in, H, m)\n",
        "            elif mod == \"ADIDA\":\n",
        "                ks = choose_adida_ks(rec_ratio)\n",
        "                yhat, meta = fc_adida(y_in, H, base_model=\"ETS\", m=m, ks=ks)\n",
        "\n",
        "                # Penalización ADIDA si es MTO\n",
        "                if is_mto(plan_row):\n",
        "                    yhat = yhat * MTO_ADIDA_GAMMA\n",
        "                    meta[\"mto_penalty\"] = MTO_ADIDA_GAMMA\n",
        "            else:\n",
        "                tried.append({\"model\":mod,\"status\":\"skip\"})\n",
        "                continue\n",
        "\n",
        "            if apply_uplift and uplift_percent and float(uplift_percent) > 0:\n",
        "                yhat = yhat * (1.0 + float(uplift_percent))\n",
        "\n",
        "            meta_out = {\"model_used\": mod, **meta,\n",
        "                        \"recency_ratio\": rec_ratio, \"recency_weight\": rec_weight,\n",
        "                        \"tried\": [mod] + [t[\"model\"] for t in tried]}\n",
        "            return yhat, meta_out\n",
        "\n",
        "        except Exception as e:\n",
        "            tried.append({\"model\":mod,\"status\":f\"fail:{e}\"})\n",
        "            continue\n",
        "\n",
        "    yhat, meta = fc_naive_seasonal(_trim(y, TRAIN_WINDOW), H, m)\n",
        "    meta[\"fallback\"] = \"all_failed->Naive\"\n",
        "    return yhat, {\"model_used\":\"Naive\", **meta,\n",
        "                  \"recency_ratio\": rec_ratio, \"recency_weight\": rec_weight,\n",
        "                  \"tried\":[t['model'] for t in tried]}\n",
        "\n",
        "def main():\n",
        "    outdir = Path(OUTDIR).resolve()\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1) Plan\n",
        "    print(\"[1/4] Leyendo plan…\")\n",
        "    plan, _ = load_plan_any(PLAN_XLSX, PLAN_SHEET, PLAN_CSV_FALLBACK)\n",
        "    if \"Material_excel\" in plan.columns:\n",
        "        plan = plan.drop(columns=[\"Material_excel\"])\n",
        "    width = infer_width_from_plan(plan)\n",
        "    plan[\"Material\"] = norm_id(plan[\"Material\"], width)\n",
        "    print(f\"[INFO] Ancho observado en plan: {width}\")\n",
        "\n",
        "    # columnas control\n",
        "    defaults = {\"candidates\":\"\", \"seasonal_periods\": DEFAULT_M, \"low_data_flag\": False,\n",
        "                \"forced_intermittent\": False, \"apply_uplift\": False, \"uplift_percent\": 0.0}\n",
        "    for k,v in defaults.items():\n",
        "        if k not in plan.columns:\n",
        "            plan[k] = v\n",
        "    def to_int_m(x):\n",
        "        try: return int(float(x))\n",
        "        except Exception: return DEFAULT_M\n",
        "    plan[\"seasonal_periods\"] = plan[\"seasonal_periods\"].apply(to_int_m)\n",
        "\n",
        "    # 2) Series\n",
        "    print(\"[2/4] Cargando/Construyendo series…\")\n",
        "    weekly = load_weekly_series()\n",
        "    # normalizar llaves a mismo ancho\n",
        "    weekly = { norm_id(pd.Series([k]), width).iloc[0]: v for k,v in weekly.items() }\n",
        "    print(f\"[INFO] Materiales en series (normalizados): {len(weekly):,}\")\n",
        "\n",
        "    mats_plan   = plan[\"Material\"].astype(str).tolist()\n",
        "    mats_series = set(weekly.keys())\n",
        "    mats = [m for m in mats_plan if m in mats_series]\n",
        "    missing = [m for m in mats_plan if m not in mats_series]\n",
        "    print(f\"[INFO] A pronosticar: {len(mats):,} | Faltan en series: {len(missing):,}\")\n",
        "    if missing:\n",
        "        print(f\"  Ejemplos faltantes: {missing[:10]}\")\n",
        "\n",
        "    # 3) Pronóstico\n",
        "    print(\"[3/4] Pronosticando…\")\n",
        "    rows, wrows = [], []\n",
        "    with tqdm(total=len(mats), desc=\"Pronosticando SKUs\", unit=\"SKU\") as pbar:\n",
        "        for _, r in plan[plan[\"Material\"].isin(mats)].iterrows():\n",
        "            mat = str(r[\"Material\"])\n",
        "            y = weekly[mat]\n",
        "            if len(y) < 4:\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix({\"material\": mat, \"status\": \"serie_corta\"})\n",
        "                continue\n",
        "\n",
        "            models = normalize_models_list(r.get(\"primary_model\",\"\"), r.get(\"candidates\",\"\"))\n",
        "            m = int(r.get(\"seasonal_periods\", DEFAULT_M) or DEFAULT_M)\n",
        "\n",
        "            low     = str(r.get(\"low_data_flag\", False)).lower() in {\"true\",\"1\",\"yes\"}\n",
        "            finterm = str(r.get(\"forced_intermittent\", False)).lower() in {\"true\",\"1\",\"yes\"}\n",
        "            ap_upl  = str(r.get(\"apply_uplift\", False)).lower() in {\"true\",\"1\",\"yes\"}\n",
        "            upc     = float(r.get(\"uplift_percent\", 0.0) or 0.0)\n",
        "\n",
        "            yhat, meta = forecast_one(y, r, models, m, low, finterm, ap_upl, upc)\n",
        "\n",
        "            # SUM/AVG base\n",
        "            yhat_sum = float(np.sum(yhat))\n",
        "            yhat_avg = float(np.mean(yhat))\n",
        "\n",
        "            # Ajuste de recencia universal (down-weight)\n",
        "            ratio, weight = meta.get(\"recency_ratio\", np.nan), meta.get(\"recency_weight\", np.nan)\n",
        "            applied_recency = False\n",
        "            if np.isfinite(ratio) and ratio > 1.0:\n",
        "                yhat_sum *= weight\n",
        "                yhat_avg *= weight\n",
        "                applied_recency = True\n",
        "            if np.isfinite(ratio) and ratio >= OBSOLETE_RATIO:\n",
        "                yhat_sum = 0.0\n",
        "                yhat_avg = 0.0\n",
        "                applied_recency = True\n",
        "\n",
        "            model_used = meta.get(\"model_used\",\"NA\")\n",
        "            if applied_recency:\n",
        "                model_used = f\"{model_used}{TAG_RECENCY_SUFFIX}\"\n",
        "\n",
        "            # Detalle semanal\n",
        "            idx_fut = pd.date_range(y.index.max() + pd.tseries.frequencies.to_offset(FREQ), periods=H, freq=FREQ)\n",
        "            for i, ts in enumerate(idx_fut):\n",
        "                wrows.append({\"Material\": mat, \"week\": ts, \"yhat\": float(yhat[i] if i < len(yhat) else 0.0)})\n",
        "\n",
        "            # Resumen\n",
        "            rows.append({\n",
        "                \"Material\": mat,\n",
        "                \"yhat_sum_H\": yhat_sum,\n",
        "                \"yhat_avg_H\": yhat_avg,\n",
        "                \"model_used\": model_used,\n",
        "                \"recency_ratio\": ratio,\n",
        "                \"recency_weight\": weight,\n",
        "                \"seasonal_periods_used\": m\n",
        "            })\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({\"material\": mat, \"modelo\": model_used})\n",
        "\n",
        "    df_weekly  = pd.DataFrame(wrows)\n",
        "    df_summary = pd.DataFrame(rows)\n",
        "\n",
        "    # 4) Guardar\n",
        "    print(\"[4/4] Guardando…\")\n",
        "    safe_to_csv(df_weekly,  Path(OUTDIR) / \"forecast_weekly.csv\")\n",
        "    safe_to_csv(df_summary, Path(OUTDIR) / \"forecast_summary.csv\")\n",
        "\n",
        "    try:\n",
        "        xlsx = Path(OUTDIR) / \"forecast_summary.xlsx\"\n",
        "        with pd.ExcelWriter(xlsx, engine=\"xlsxwriter\") as writer:\n",
        "            df_summary.to_excel(writer, index=False, sheet_name=\"Resumen\")\n",
        "            wb, ws = writer.book, writer.sheets[\"Resumen\"]\n",
        "            text_fmt = wb.add_format({'num_format':'@'})\n",
        "            col_idx = list(df_summary.columns).index('Material')\n",
        "            ws.set_column(col_idx, col_idx, 22, text_fmt)\n",
        "        print(f\"[OK] XLSX guardado: {xlsx.resolve()}\")\n",
        "        open_in_excel(xlsx)\n",
        "    except Exception as e:\n",
        "        print(f\"[AVISO] No se pudo escribir/abrir XLSX: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}